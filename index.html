
<!-- saved from url=(0046)https://mm2022-apccpa-workshop.github.io/#call -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <!-- SITE TITTLE -->
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Workshop LGM3A 2024</title>
    
    <!-- PLUGINS CSS STYLE -->
    <!-- link rel="icon" href="https://2022.acmmm.org/wp-content/themes/acmmultimedia/assets/images/favicon-acm.png" -->
    <link href="./LGM3A Workshop _ ACM MM 2024_files/jquery-ui.min.css" rel="stylesheet">
    <!-- Bootstrap -->
    <link href="./LGM3A Workshop _ ACM MM 2024_files/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome -->
    <link href="./LGM3A Workshop _ ACM MM 2024_files/font-awesome.min.css" rel="stylesheet">
    <!-- Owl Carousel -->
    <link href="./LGM3A Workshop _ ACM MM 2024_files/slick.css" rel="stylesheet">
    <link href="./LGM3A Workshop _ ACM MM 2024_files/slick-theme.css" rel="stylesheet">
    <!-- Fancy Box -->
    <link href="./LGM3A Workshop _ ACM MM 2024_files/jquery.fancybox.pack.css" rel="stylesheet">
    <link href="./LGM3A Workshop _ ACM MM 2024_files/nice-select.css" rel="stylesheet">
    <link href="./LGM3A Workshop _ ACM MM 2024_files/bootstrap-slider.min.css" rel="stylesheet">
    <!-- CUSTOM CSS -->
    <link href="./LGM3A Workshop _ ACM MM 2024_files/style.css" rel="stylesheet">
    
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
      <![endif]-->
   <style type="text/css">
    .left,
    .right {
      float: left;
      width: 50%;
      padding-left: 80px;
    }
    .left { padding-left: 250px; }
  </style> 
    </head>
    
    <body class="body-wrapper" data-new-gr-c-s-check-loaded="14.997.0" data-gr-ext-installed="" style="background:hsl(108, 41%, 73%);">
        <section>
          <div class="container">
            <div class="row">
              <div class="col-md-12">
                <nav class="navbar navbar-expand-lg  navigation"> <a class="navbar-brand" href="https://www.acmmm2023.org/"> <img src="./LGM3A Workshop _ ACM MM 2024_files/logo-acm-2024.png" alt="" width="100px"> </a>
                  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"> <span class="navbar-toggler-icon"></span> </button>
                  <div class="collapse navbar-collapse" id="navbarSupportedContent">
                    <ul class="navbar-nav ml-auto main-nav">
                      <li class="nav-item"> <a class="nav-link" href="https://lgm3a.github.io/LGM3A2024/#overview"><font color="white"><strong>Home</strong></font></a></li>
					  
					  <li class="nav-item"> <a class="nav-link " href="https://lgm3a.github.io/LGM3A2024/#announcements"><font color="white"><strong>News</strong></font></a></li>

                      <li class="nav-item"> <a class="nav-link " href="https://lgm3a.github.io/LGM3A2024/#call"><font color="white"><strong>Call for papers</strong></font></a></li>
      
					  <li class="nav-item"> <a class="nav-link" href="https://lgm3a.github.io/LGM3A2024/#submission"><font color="white"><strong>Submission</strong></font></a></li>
	  
                      <li class="nav-item"> <a class="nav-link" href="https://lgm3a.github.io/LGM3A2024/#organizers"><font color="white"><strong>Organizers</strong></font></a></li>

                      <li class="nav-item"> <a class="nav-link" href="https://lgm3a.github.io/LGM3A2024/#speakers"><font color="white"><strong>Speakers</strong></font></a></li>
    
                      <li class="nav-item"> <a class="nav-link" href="https://lgm3a.github.io/LGM3A2024/#schedule"><font color="white"><strong>Schedule</strong></font></a></li>
                    </ul>
                    
                  </div>
                </nav>
              </div>
            </div>
          </div>
        </section>
    
    
    <!--==========================================
    =            Overview Section            =
    ===========================================-->
    
    <section class="popular-deals section bg-white"> 
      <!-- Container Start -->
      <div class="container">
        <div class="row">
          <div class="col-md-12"> 
            <!-- Section title -->
            <div class="section-title">
              <h1>Large Generative Models Meet Multimodal Applications (LGM3A)</h1>
			  <br>
			  <h4>Workshop at ACM Multimedia 2024</h4>
			  <br><br>
              <h2 id="overivew">Scope and Topics</h2>
            </div>
        This workshop aims to explore the potential of large generative models to revolutionize the way we interact with multimodal information. 
        A Large Language Model (LLM) represents a sophisticated form of artificial intelligence engineered to comprehend and produce natural language text, exemplified by technologies such as GPT, LLaMA, Flan-T5, ChatGLM, and Qwen, etc. 
        These models undergo training on extensive text datasets, exhibiting commendable attributes including robust language generation, zero-shot transfer capabilities, and In-Context Learning (ICL). 
        With the surge in multimodal content—encompassing images, videos, audio, and 3D models—over the recent period, Large MultiModal Models (LMMs) have seen significant enhancements. 
        These improvements enable the augmentation of conventional LLMs to accommodate multimodal inputs or outputs, as seen in BLIP, Flamingo, KOSMOS, LLaVA, Gemini, GPT-4, etc. 
        Concurrently, certain research initiatives have delved into generating specific modalities, with Kosmos2 and MiniGPT-5 focusing on image generation, and SpeechGPT on speech production. 
        There are also endeavors to integrate LLMs with external tools to achieve a near 'any-to-any' multimodal comprehension and generation capacity, illustrated by projects like Visual-ChatGPT, ViperGPT, MMREACT, HuggingGPT, and AudioGPT. 
        Collectively, these models, spanning not only text and image generation but also other modalities, are referred to as large generative models. 
        This workshop will provide an opportunity for researchers, practitioners, and industry professionals to explore the latest trends and best practices in the field of multimodal applications of large generative models. 
        We also remark that the submissions are not limited to the use of such models. The workshop will also focus on exploring the challenges and opportunities of integrating large language models with other AI technologies such as computer vision and speech recognition. 
        Additionally, the workshop will provide a platform for participants to present their research, share their experiences, and discuss potential collaborations. 
        </div>
        </div>
      </div>
    </section>
    

    
    <!--===========================================
    =            Dates Section            =
    ============================================-->

    <section class="section bg-white"> 
      <div class="container">
        <div class="row">
          <div class="col-12"> 
            <div class="section-title">
              <h2 id="announcements">News</h2>
            </div>
			<ul style="margin-left: 40px">
      <li>28/8/2023 - Workshop schedule is announced. </li>
      <li>8/8/2023 - Workshop papers notification is announced. </li>
      <li>19/7/2023 - Workshop papers submission is delayed. </li>
      <li>27/4/2023 - Important dates are updated. </li>
      <li>6/4/2023 - CFP is released. </li>
	  <li>6/4/2023 - Workshop homepage is now available. </li>
			</ul>

          </div>
        </div>
      </div>
    </section>

	
	<!--==========================================
    =            Call for Papers Section            =
    ===========================================-->
    
    <section class="section bg-white"> 
      <div class="container">
        <div class="row">
          <div class="col-12"> 
            <div class="section-title">
              <h2 id="call">Call for Papers</h2>
            </div>
           This workshop intends to 1) provide a platform for researchers to present their latest works and receive feedback from experts in the field, 
		   2) foster discussions on current challenges and opportunities in multimodal analysis and application, 
		   3) identify emerging trends and opportunities in the field, and 
		   4) explore their potential impact on future research and development. Potential topics include, but are not limited to:
			<ul style="margin-left: 40px">

      <li>Multimodal data augmentation</li>
      <li>Multimodal data analysis and understanding</li>
      <li>Multimodal question answering</li>
      <li>Multimodal generation</li>
      <li>Multimodal retrieval augmentation</li>
      <li>Multimodal recommendation </li>
      <li>Multimodal summarization and text generation</li>
      <li>Multimodal agents</li>
      <li>Multimodal prompting</li>
      <li>Multimodal continual learning</li>
      <li>Multimodal fusion and integration of information</li>
      <li>Multimodal applications/pipelines</li>
      <li>Multimodal systems management and indexing</li>
      <li>Multimodal mobile/lightweight deployment</li>
			</ul>

      Important dates: 
			<ul style="margin-left: 40px">

        <li>Workshop Papers Submission: <strong>July 19, 2024</strong></li>
	    <li>Workshop Papers Notification: <strong>August 5, 2024</strong></li>
        <li>Camera-ready Submission: <strong>August 19, 2024</strong></li>
		<li>Conference dates: 28 October - 1 November 2024</li>
        </ul>
		Please note: The submission deadline is at 11:59 p.m. of the stated deadline date&nbsp;<a href="https://time.is/Anywhere_on_Earth"><font color="red"><u>Anywhere on Earth</u></font></a>.</p>

          </div>
        </div>
      </div>
    </section>

    <!--==========================================
    =            Papers Section            =
    ===========================================-->
    
    <section class="section bg-white"> 
      <div class="container">
        <div class="row">
          <div class="col-12"> 
            <div class="section-title">
              <h2 id="submission">Submission</h2>
            </div>
            <li><strong>Submission Guidelines</strong>:</li>
			Submitted papers (.pdf format) must use the <a href="https://www.acm.org/publications/proceedings-template"><font color=#5672f9><u>ACM Article Template</u></font></a>. Please remember to add Concepts and Keywords.
			Submissions can be of varying length from <strong>4</strong> to <strong>8</strong> pages, plus additional pages for the reference pages, i.e., the reference page(s) are not counted to the page limit of 4 to 8 pages. There is no distinction between long and short papers, but the authors may themselves decide on the appropriate length of the paper. All papers will undergo the same review process and review period.
            Paper submissions must conform with the <strong>"double-blind"</strong> review policy. All papers will be peer-reviewed by experts in the field. Acceptance will be based on relevance to the workshop, scientific novelty, and technical quality. The workshop papers will be published in the ACM Digital Library. 
			<li><strong>Submission Site</strong>:
			<a href="https://easychair.org/conferences/?conf=lgm3a2023"><font color=#5672f9><u>https://easychair.org/conferences/?conf=lgm3a2023</u></font></a></li>
          </div>
        </div>
      </div>
    </section>	



	
    <!--==========================================
    =            Organizers Section            =
    ===========================================-->
    
    <section class="section bg-white"> 
      <div class="container">
        <div class="row">
          <div class="col-12"> 
            <div class="section-title">
              <h2 id="organizers">Organizers</h2>
            </div>
			<ul style="margin-left: 40px">
			<li><a href="https://zhengwang125.github.io/"><font color=#5672f9><u>Zheng Wang</u></font></a> (Huawei Singapore Research Center, Singapore)</li>
			<li><a href="https://personal.ntu.edu.sg/c.long/index.html"><font color=#5672f9><u>Cheng Long</u></font></a> (Nanyang Technological University, Singapore)</li>
			<li><font color=#5672f9><u>Shihao Xu</u></font> (Huawei Singapore Research Center, Singapore)</li>
			<li><font color=#5672f9><u>Bingzheng Gan</u></font> (Huawei Singapore Research Center, Singapore)</li>
			<li><font color=#5672f9><u>Wei Shi</u></font> (Huawei Singapore Research Center, Singapore)</li>
			<li><a href="https://scholar.google.com/citations?user=aJmTPaoAAAAJ&hl=en"><font color=#5672f9><u>Zhao Cao</u></font></a> (Huawei Technologies Co., Ltd, China)</li>
			<li><a href="https://www.chuatatseng.com/"><font color=#5672f9><u>Tat-Seng Chua</u></font></a> (National University of Singapore, Singapore)</li>
			</ul>

    <!--==========================================
    =            Speakers Section            =
    ===========================================-->
    
    <section class="section bg-white"> 
      <div class="container">
        <div class="row">
          <div class="col-12"> 
            <div class="section-title">
              <h2 id="speakers">Speakers</h2>
            </div>

                 
            <h3><b>Keynote 1</b></h3> 
                    
            <!-- <a href="https://lme.tf.fau.de/person/maier/" style="color: blueviolet;"> -->
            
            <img src="https://cde.nus.edu.sg/ece/wp-content/uploads/sites/3/2021/05/zhengshou_stanford_photo.jpg" class="media-left pull-left" width="170" height="180" alt="" margin="10" hspace="10" vspace="5"> 
            </a> 

            <a href="https://sites.google.com/view/showlab"><font color=#5672f9><u>Prof. Mike Zheng Shou</u></font></a> is a tenure-track Assistant Professor at National University of Singapore and a former Research Scientist at Facebook AI in the Bay Area. He holds a PhD degree from Columbia University in the City of New York, where he worked with Prof. Shih-Fu Chang. He was awarded the Wei Family Private Foundation Fellowship. He received the best paper finalist at CVPR'22 and the best student paper nomination at CVPR'17. His team won 1st place in multiple international challenges including ActivityNet 2017, EPIC-Kitchens 2022, Ego4D 2022 & 2023. He is a Fellow of the National Research Foundation (NRF) Singapore and has been named on the Forbes 30 Under 30 Asia list. 
            
            </p>

            <b>Talk Title:</b> Large Generative Models Meet Multimodal Video Intelligence
            </br>
            <b>Abstract:</b> In this talk, I'd like to share my recent research around multimodal video intelligence in the era of large generative models. I will first talk about video-language pretraining techniques (All-in-one, EgoVLP) that use one single model to power various understanding tasks ranging from retrieval to QA. Then I will introduce challenges and our efforts of adapting these large pretrained models to AI Assistant, such a real-world application (AssistQ, AssistGPT). Finally I will delve into the reverse problem i.e. given open-world textual description, how to generate videos (Tune-A-Video, Show-1).
            
            <br></br>

            <h3><b>Keynote 2</b></h3> 
                    
            <!-- <a href="https://lme.tf.fau.de/person/maier/" style="color: blueviolet;"> -->
            
            <img src="https://www.boyangli.org/images/face.jpg" class="media-left pull-left" width="155" alt="" margin="10" hspace="10" vspace="5"> 
            </a> 

            <a href="http://www.boyangli.org/"><font color=#5672f9><u>Prof. Boyang Li</u></font></a> is a Nanyang Associate Professor at the School of Computer Science and Technology, Nanyang Technological University. His research interests lie in computational narrative intelligence, multimodal learning, and machine learning. In 2021, he received the
            National Research Foundation Fellowship, a prestigious research award of 2.5 million Singapore Dollars. Prior to that, he worked as a senior research scientist at Baidu Research USA and a research scientist
            at Disney Research Pittsburgh, where he led an independent research group. He received his Ph.D. degree from Georgia Institute of Technology in 2015 and his Bachelor degree from Nanyang Technological University in 2008. He currently serves as a senior
            action editor for ACL Rolling Review and an associate editor for IEEE Transactions on Audio, Speech and Language Processing. His work has been reported by international media outlets such as the Guardian, New Scientist, US National Public Radio, Engadget,
            TechCrunch, and so on.
            
            </p>

            <b>Talk Title:</b> Unlocking Multimedia Capabilities of Gigantic Pretrained Language Models
            </br>
            <b>Abstract:</b> A large language model (LLM) can be analogized to an enormous treasure box guarded by a lock. It contains extensive knowledge, but applying appropriate knowledge to solve the problem at hand requires special techniques. In this talk, I will discuss techniques to unlock the capability of LLMs to process both visual and linguistic information. VisualGPT is one of the earliest works that finetunes an LLM for a vision-language task. InstructBLIP is an instruction-tuned large vision-language model, which set new states of the art on several vision-language tasks. 
            In addition, I will talk about how to unlock zero-shot capabilities without end-to-end finetuning. Plug-and-Play VQA and Img2LLM achieve excellent results on visual question-answering by simply connecting existing pretrained networks using natural language and model interpretations. Finally, I will describe a new multimodal dataset, Synopses of Movie Narratives, or SyMoN, for story understanding. I will argue that story understanding is an important objective in the pursuit of artificial general intelligence (AGI). Compared to other multimodal story datasets, the special advantages of SyMoN include (1) event descriptions at the right level of granularity, (2) abundant mental state descriptions, (3) the use of diverse storytelling techniques, and (4) the provision of easy-to-use automatic performance evaluation.
            

            <br></br>

            <h3><b>Keynote 3</b></h3> 
                    
            <!-- <a href="https://lme.tf.fau.de/person/maier/" style="color: blueviolet;"> -->
            
            <img src="https://liuziwei7.github.io/homepage_files/me.png" class="media-left pull-left" width="170" alt="" margin="10" hspace="10" vspace="5"> 
            </a> 

            <a href="https://liuziwei7.github.io/"><font color=#5672f9><u>Prof. Ziwei Liu</u></font></a> is currently a Nanyang Assistant Professor at Nanyang Technological University, Singapore. His research revolves around computer vision, machine learning and computer graphics. He has published extensively on top-tier conferences and journals in relevant fields, including CVPR, ICCV, ECCV, NeurIPS, ICLR, SIGGRAPH, TPAMI, TOG and Nature - Machine Intelligence, with around 30,000 citations. He is the recipient of Microsoft Young Fellowship, Hong Kong PhD Fellowship, ICCV Young Researcher Award, HKSTP Best Paper Award, CVPR Best Paper Award Candidate, WAIC Yunfan Award and ICBS Frontiers of Science Award. He has won the championship in major computer vision competitions, including DAVIS Video Segmentation Challenge 2017, MSCOCO Instance Segmentation Challenge 2018, FAIR Self-Supervision Challenge 2019, Video Virtual Try-on Challenge 2020 and Computer Vision in the Wild Challenge 2022. He is also the lead contributor of several renowned computer vision benchmarks and softwares, including CelebA, DeepFashion, MMHuman3D and MMFashion. He serves as an Area Chair of CVPR, ICCV, NeurIPS and ICLR, as well as an Associate Editor of IJCV.
            
            </p>

            <b>Talk Title:</b> Multi-Modal Generative AI with Foundation Models
            </br>
            <b>Abstract:</b> Generating photorealistic and controllable visual contents has been a long-pursuing goal of artificial intelligence (AI), with extensive real-world applications. It is also at the core of embodied intelligence. In this talk, I will discuss our work in AI-driven visual context generation of humans, objects and scenes, with an emphasis on combing the power of neural rendering with large multimodal foundation models. Our generative AI framework has shown its effectiveness and generalizability on a wide range of tasks. 


    <!--===========================================
    =            Workshop Arrangement            =
    ============================================-->
    
    <section class="popular-deals section bg-white">
      <div class="container">
        <div class="row">
          <div class="col-md-12">
            <div class="section-title">
              <h2 id="schedule">Workshop Schedule</h2>
            </div>
            <p align="center">
                <!-- <img src="./res/schedule.png" width="90%"> -->
                <!-- <big><b>TBD</b></big> -->
                <!-- <font color="Blue"><i><strong>TBA </strong></i></font> -->
                <li>On-site venue: TBD</li>
                <li>Date & time: TBD</li>
                <li>Zoom link: <a href="TBD"><font color=#5672f9><u>TBD</u></font></a> 
                 <!-- (Meeting ID: 892 7684 0596, Passcode: 080304)  --> </li>
                  <br/>
                  <style type="text/css">
                    #myta td{
                      padding-right: 8px;
                    }
                  </style>
                  <table border="1" id="myta">

                    <tr> <td><strong>Time</strong></td> <td>	<strong>Title</strong>	</td></tr>
                    <tr> <td>09:00 AM - 09:10 AM</td> <td>	Welcome Message from the Chairs	</td></tr>
                    <tr> <td>09:10 AM - 09:40 AM</td> <td>	Keynote 1	</td></tr>
                    <tr> <td>09:40 AM - 10:10 AM</td> <td>	Keynote 2 </td></tr>
                    <tr> <td>10:10 AM - 10:30 AM</td> <td>	Coffee Break </td></tr>
                    <tr> <td>10:30 AM - 11:00 AM</td> <td>	Presentation 1 </td></tr>
                    <tr> <td>11:00 AM - 11:30 AM</td> <td>	Presentation 2 </td></tr>
                    <tr> <td>11:30 AM - 12:00 PM</td> <td>	Presentation 3 </td></tr>
                    <tr> <td>12:00 PM - 12:10 PM</td> <td>	Workshop Closing </td></tr>
                    </table>
<br>
<br>
                    <!--
                    <table border="1">
                    <tbody><tr> <td><strong>Time (October 14, Local Portugal Time)</strong></td> <td>	<strong>Paper Title</strong>	</td></tr>
                    <tr> <td>14:00 PM - 14:15 PM</td> <td>	IPDAE: Improved Patch-Based Deep Autoencoder for Lossy Point Cloud Geometry Compression	</td></tr>
                    <tr> <td>14:15 PM - 14:30 PM</td> <td>	GRASP-Net: Geometric Residual Analysis and Synthesis for Point Cloud Compression	</td></tr>
                    <tr> <td>14:30 PM - 14:45 PM</td> <td>	Wiener Filter-Based Point Cloud Adaptive Denoising for Video-based Point Cloud Compression	</td></tr>
                    <tr> <td>14:45 PM - 15:00 PM</td> <td>	OpenPointCloud-V2: A Deep Learning Based Open-Source Algorithm Library of Point Cloud Processing	</td></tr>

                    <tr> <td>15:00 PM - 15:15 PM</td> <td>	End-to-End Point Cloud Geometry Compression and Analysis with Sparse Tensor	</td></tr>
                    <tr> <td>15:15 PM - 15:30 PM</td> <td>	Transformer and Upsampling-Based Point Cloud Compression	</td></tr>
                    <tr> <td>15:30 PM - 15:45 PM</td> <td>	Quality Evaluation of Machine Learning-based Point Cloud Coding Solutions	</td></tr>
                    <tr> <td>15:45 PM - 16:00 PM</td> <td>	View-Adaptive Streaming of Point Cloud Scenes through Combined Decomposition and Video-based Coding	</td></tr>
                    </tbody></table> -->
				
            </p>
          </div>
        </div>
      </div>
    </section>


    
    <!--============================
    =            Footer            =
    =============================-->
    
    <!-- Footer Bottom -->
    <!-- footer class="footer-bottom"> 
      <!-- Container Start -->
      <div class="container">
        <div class="row">
          <div class="col-sm-12 col-12"> 
            <!-- Copyright -->
            <div class="copyright">
              <p class="text-center">
                For any questions, please email to <a href="mailto:LGM3A2024@gmail.com><font color=#5672f9><u>LGM3A2024@gmail.com</u></a>
              </p>
                <br class="clear">
            </div>
          </div>
        </div>
      </div>
      <!-- Container End --> 
    </footer>
    
    <!-- JAVASCRIPTS --> 
    <script src="./LGM3A Workshop _ ACM MM 2024_files/jquery.min.js.download"></script> 
    <script src="./LGM3A Workshop _ ACM MM 2024_files/jquery-ui.min.js.download"></script> 
    <script src="./LGM3A Workshop _ ACM MM 2024_files/tether.min.js.download"></script> 
    <script src="./LGM3A Workshop _ ACM MM 2024_files/jquery.raty-fa.js.download"></script> 
    <script src="./LGM3A Workshop _ ACM MM 2024_files/popper.min.js.download"></script> 
    <script src="./LGM3A Workshop _ ACM MM 2024_files/bootstrap.min.js.download"></script> 
    <script src="./LGM3A Workshop _ ACM MM 2024_files/bootstrap-slider.min.js.download"></script> 
    <script src="./LGM3A Workshop _ ACM MM 2024_files/slick.min.js.download"></script> 
    <script src="./LGM3A Workshop _ ACM MM 2024_files/jquery.nice-select.min.js.download"></script> 
    <script src="./LGM3A Workshop _ ACM MM 2024_files/jquery.fancybox.pack.js.download"></script> 
    <script src="./LGM3A Workshop _ ACM MM 2024_files/SmoothScroll.min.js.download"></script> 
    <script src="./LGM3A Workshop _ ACM MM 2024_files/scripts.js.download"></script -->
    
    
    

</body></html>
